---
layout: page
title: Review
---


I review various fields of machine learning works.



# Optimization

[O1] Understanding the weight decay [[Slides](https://Kthyeon.github.io/reviews/weight_decay/Weight_decay_KTH.pdf)]

 - [[NeurIPS2020](https://proceedings.neurips.cc//paper/2020/file/32fcc8cfe1fa4c77b5c58dafd36d1a98-Paper.pdf)] On the training dynamics of deep networks with L2 regularization 

[O2] Dynamic sparse reprarameterization [[Slides](https://Kthyeon.github.io/reviews/dynamic_sparse_reparameterization/presentation.pdf)]

- [[ICML2019](http://proceedings.mlr.press/v97/mostafa19a/mostafa19a.pdf)] Parameter Efficient Training of Deep Convolutional Neural Networks by Dynamic Sparse Reparameterization

[O3] Loss change allocation for neural network training [[Slides](https://Kthyeon.github.io/reviews/Loss_Change_Allocation_for_Neural_Network_Training/20200102_seminar.pdf)]

- [[NeurIPS2019](https://openreview.net/pdf?id=rkepv4HlLH)] LCA: Loss Change Allocation for Neural Network Training

[O4] How SGD Selects the Global Minima in Over-Parameterized Learning A Dynamical Stability Perspective [[Slides](https://Kthyeon.github.io/reviews/how_sgd_global_minima/0919_presentation.pdf)]

- [[NeurIPS2019](https://papers.nips.cc/paper/2018/file/6651526b6fb8f29a00507de6a49ce30f-Paper.pdf)] How SGD Selects the Global Minima in Over-parameterized Learning: A Dynamical Stability Perspective

[O5] Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation [[Slides](https://Kthyeon.github.io/reviews/comparison_kd/Seminar.pdf)]

- [[IJCAI2021](https://arxiv.org/abs/2105.08919)] Comparing Kullback-Leibler Divergence and Mean Squared Error Loss in Knowledge Distillation

[O6] Orthogonality in Convolutional Neural Networks [[Slides](https://Kthyeon.github.io/reviews/cnn_orthogonality/CNN_orthogonality.pdf)]

- [[CVPR2020](https://www1.icsi.berkeley.edu/~stellayu/publication/doc/2020ocnnCVPR.pdf)] Orthogonal Convolutional Neural Networks
- [[ICML2020](https://arxiv.org/abs/2006.16992)] Deep Isometric Learning for Visual Recognition

[O7] Can We Gain more from [[Slides](https://Kthyeon.github.io/reviews/Can_We_Gain_More_from_Orthogonality_Regularizations/seminar.pdf)]

- [[NeurIPS2019](https://papers.nips.cc/paper/2018/file/bf424cb7b0dea050a42b9739eb261a3a-Paper.pdf)] Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?

# Federated Learning

[F1] Overview for Federated Learning [[Slides](https://Kthyeon.github.io/reviews/talk_for_fl/200710_FL_Taehyeon.pdf)]

- [[ICLR2021](https://openreview.net/forum?id=LkFG3lB13U5)] Adaptive Federated Optimization
- [[ICML2019](http://proceedings.mlr.press/v97/mohri19a/mohri19a.pdf)] Agnostic Federated Learning

[F2] Anaylsis of Federated Learning [[Slides](https://Kthyeon.github.io/reviews/fl_on_the_convergence/Presentation_FL_seminar_0410.pdf)]

- [[AISTATS2017](http://proceedings.mlr.press/v54/mcmahan17a/mcmahan17a.pdf)] Communication-Efficient Learning of Deep Networks from Decentralized Data
- [[ICLR2020](https://openreview.net/pdf?id=HJxNAnVtDS)] ON THE CONVERGENCE OF FEDAVG ON NON-IID DATA

# AutoML

[A1] Scalabel Global Optimization via Local Bayesian Optimization [[Slides](https://Kthyeon.github.io/reviews/scalable_bayesian/200904_seminar_TH.pdf)]

- [[NeurIPS2019](https://papers.nips.cc/paper/2019/file/6c990b7aca7bc7058f5e98ea909e924b-Paper.pdf)] Scalabel Global Optimization via Local Bayesian Optimization


# Trustworthy and real-world AI/ML challenges

[TR1] Multi-armed bandits with compensation [[Slides](https://Kthyeon.github.io/reviews/Multi_armed_Bandits_with_Compensation/seminar.pdf)]

- [[NeurIPS2018](https://proceedings.neurips.cc/paper/2018/file/8bdb5058376143fa358981954e7626b8-Paper.pdf)] Multi-armed Bandits with Compensation

[TR2] Introduction to multi-armed bandit [[Slides](https://Kthyeon.github.io/reviews/Introduction_to_multi_armed_bandit/seminar.pdf)]

- [[Materials](https://Kthyeon.github.io/reviews/Introduction_to_multi_armed_bandit/studymaterial/MAB-book.pdf)] Introduction to Multi-Armed Bandits

[TR3] Tricks for Image Classification [[Slides](https://Kthyeon.github.io/reviews/Image_classification/seminar.pdf)]

- Cutout, Mixup, DropBlock, AutoAugment

[TR4] Contextual Gaussian Process bandit optimization [[Slides](https://Kthyeon.github.io/reviews/Contextual_gaussian_process_bandit_optimization/seminar.pdf)]

- [[NIPS2011](https://papers.nips.cc/paper/2011/file/f3f1b7fc5a8779a9e618e1f23a7b7860-Paper.pdf)] Contextual Gaussian Process Bandit Optimization

[TR5] Asymptotically Efficient Allocation Rules [[Slides](https://Kthyeon.github.io/reviews/Asymptotically_Efficient_Allocation_Rules/seminar.pdf)]

- [[Advances in Applied Mathematics 1985](https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.674.1620&rep=rep1&type=pdf)] Asymptotically Efficient Adaptive Allocation Rules

[TR6] An Empirical Evaluation of Thompson Sampling [[Slides](https://Kthyeon.github.io/reviews/empirical_ts/seminar.pdf)]

- [[NIPS2011](https://papers.nips.cc/paper/2011/file/e53a0a2978c28872a4505bdb51db06dc-Paper.pdf)] An Empirical Evaluation of Thompson Sampling



